"""
Enhanced Document Processor with Better Metadata Extraction
Optimized for Vietnamese textbooks (especially Tin h·ªçc)
"""

import logging
import re
import argparse
from pathlib import Path
from typing import List, Optional, Tuple, Dict
import tiktoken
from langchain_text_splitters import RecursiveCharacterTextSplitter

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)


class DocumentProcessor:
    """Enhanced Document Processor for Vietnamese educational content"""

    def __init__(
        self,
        subject: Optional[str] = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        encoding_name: str = "cl100k_base",
        smart_chunking: bool = True
    ):
        self.subject_key = subject
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.encoding = tiktoken.get_encoding(encoding_name)
        self.smart_chunking = smart_chunking
        logger.info(f"DocumentProcessor initialized (subject={subject or 'auto'}, smart_chunking={smart_chunking})")

    def process_txt(self, txt_path: str | Path, output_dir: Optional[str | Path] = None) -> List[Dict]:
        """Process a single TXT file with enhanced metadata extraction"""
        txt_path = Path(txt_path)
        logger.info(f"üìÑ Processing TXT: {txt_path.name}")

        if not txt_path.exists():
            raise FileNotFoundError(f"TXT not found: {txt_path}")

        with open(txt_path, "r", encoding="utf-8") as f:
            full_text = f.read().strip()

        if not full_text:
            logger.warning(f"‚ö†Ô∏è Empty file: {txt_path.name}")
            return []

        # Detect subject & grade tr∆∞·ªõc
        subject_info = self._detect_subject(txt_path.name, full_text)
        grade, education_level = self._detect_grade(txt_path.name)

        # Lo·∫°i b·ªè ph·∫ßn ƒë·∫ßu s√°ch (l·ªùi n√≥i ƒë·∫ßu, h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng)
        full_text = self._remove_intro_sections(full_text)

        # Lo·∫°i b·ªè m·ª•c l·ª•c
        full_text = self._remove_table_of_contents(full_text)

        # Clean up text
        full_text = self._preprocess_text(full_text)

        # Split th√†nh c√°c b√†i h·ªçc ri√™ng bi·ªát
        lessons = self._split_into_lessons(full_text)
        logger.info(f"üß© Detected {len(lessons)} lessons")

        all_chunks = []
        chunk_id = 0

        # üî• Bi·∫øn ƒë·ªÉ "nh·ªõ" ch·ªß ƒë·ªÅ hi·ªán t·∫°i
        current_chapter = None
        current_chapter_number = None
        current_chapter_title = None

        for lesson_text in lessons:
            # Extract metadata cho t·ª´ng b√†i
            metadata = self._extract_lesson_metadata(lesson_text, subject_info, grade, education_level)

            # üî• N·∫øu b√†i n√†y c√≥ ch·ªß ƒë·ªÅ m·ªõi, c·∫≠p nh·∫≠t current_chapter
            if metadata.get('chapter'):
                current_chapter = metadata['chapter']
                current_chapter_number = metadata.get('chapter_number')
                current_chapter_title = metadata.get('chapter_title')
            # üî• N·∫øu kh√¥ng c√≥, d√πng ch·ªß ƒë·ªÅ tr∆∞·ªõc ƒë√≥
            else:
                metadata['chapter'] = current_chapter
                metadata['chapter_number'] = current_chapter_number
                metadata['chapter_title'] = current_chapter_title

            logger.info(f"  üìö {metadata.get('chapter') or 'N/A'} | "
                       f"B√†i {metadata.get('lesson_number')}: {metadata.get('lesson_title') or 'N/A'}")

            # Chunk b√†i h·ªçc theo sections n·∫øu c√≥
            if self.smart_chunking and metadata.get('sections'):
                chunks = self._smart_chunk_by_sections(lesson_text, metadata)
            else:
                chunks = self._basic_chunk(lesson_text, metadata)

            # G√°n chunk_id
            for chunk in chunks:
                chunk['chunk_id'] = f"{txt_path.stem}_{chunk_id:04d}"
                chunk['source_file'] = txt_path.name
                chunk_id += 1
                all_chunks.append(chunk)

        avg_tokens = sum(c['token_count'] for c in all_chunks) // len(all_chunks) if all_chunks else 0
        logger.info(f"‚úÖ Created {len(all_chunks)} chunks (avg {avg_tokens} tokens/chunk)")

        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / f"{txt_path.stem}_chunks.json"
            self._save_chunks(all_chunks, output_path)
            logger.info(f"üíæ Saved chunks to {output_path}")

        return all_chunks

    def _remove_intro_sections(self, text: str) -> str:
        """Lo·∫°i b·ªè ph·∫ßn h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng s√°ch, l·ªùi n√≥i ƒë·∫ßu"""
        patterns = [
            r"H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG S√ÅCH[\s\S]*?(?=CH·ª¶ ƒê·ªÄ|M·ª§C L·ª§C)",
            r"L·ªúI N√ìI ƒê·∫¶U[\s\S]*?(?=CH·ª¶ ƒê·ªÄ|M·ª§C L·ª§C)",
        ]
        for pattern in patterns:
            text = re.sub(pattern, "", text, flags=re.IGNORECASE)
        return text

    def _remove_table_of_contents(self, text: str) -> str:
        """Lo·∫°i b·ªè m·ª•c l·ª•c - t·ª´ 'M·ª§C L·ª§C' ƒë·∫øn tr∆∞·ªõc 'CH·ª¶ ƒê·ªÄ' ƒë·∫ßu ti√™n"""
        # T√¨m ph·∫ßn M·ª§C L·ª§C v√† b·ªè ƒëi cho ƒë·∫øn CH·ª¶ ƒê·ªÄ ƒë·∫ßu ti√™n
        toc_pattern = r"M·ª§C L·ª§C[\s\S]*?(?=CH·ª¶ ƒê·ªÄ\s+\d+\.)"
        text = re.sub(toc_pattern, "", text, flags=re.IGNORECASE)
        return text

    def _preprocess_text(self, text: str) -> str:
        """Clean up text"""
        # B·ªè page numbers
        text = re.sub(r"(?:TRANG|Trang|Page)\s+\d+", "", text, flags=re.IGNORECASE)
        # B·ªè d·∫•u g·∫°ch ngang ph√¢n trang
        text = re.sub(r"^[-=_]{3,}$", "", text, flags=re.MULTILINE)
        # Normalize whitespace
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r" {2,}", " ", text)
        return text.strip()

    def _split_into_lessons(self, text: str) -> List[str]:
        """
        Split text th√†nh c√°c b√†i h·ªçc ri√™ng bi·ªát
        M·ªói b√†i b·∫Øt ƒë·∫ßu b·∫±ng 'B√ÄI <s·ªë>:'
        """
        # Pattern ƒë·ªÉ t√¨m ƒë·∫ßu m·ªói b√†i: "B√ÄI 1:", "B√ÄI 2:", etc.
        lesson_pattern = r"(?=B√ÄI\s+\d+[:Ôºö])"

        parts = re.split(lesson_pattern, text, flags=re.IGNORECASE)

        # L·ªçc b·ªè ƒëo·∫°n r·ªóng ho·∫∑c qu√° ng·∫Øn
        lessons = [p.strip() for p in parts if len(p.strip()) > 200]

        return lessons

    def _extract_lesson_metadata(
        self,
        lesson_text: str,
        subject_info: Dict,
        grade: Optional[int],
        education_level: Optional[str]
    ) -> Dict:
        """Extract ƒë·∫ßy ƒë·ªß metadata t·ª´ m·ªôt b√†i h·ªçc"""

        metadata = {
            **subject_info,
            "grade": grade,
            "education_level": education_level,
        }

        # 1. Detect Ch·ªß ƒë·ªÅ (chapter)
        chapter_match = re.search(
            r"(CH·ª¶ ƒê·ªÄ|CHU DE)\s+(\d+)[\.\s:Ôºö-]*([^\n\r]+)",
            lesson_text,
            re.IGNORECASE
        )
        if chapter_match:
            chapter_num = int(chapter_match.group(2))
            chapter_title = re.sub(r'\s+', ' ', chapter_match.group(3).strip()).title()
            metadata["chapter_number"] = chapter_num
            metadata["chapter_title"] = chapter_title
            metadata["chapter"] = f"Ch·ªß ƒë·ªÅ {chapter_num}. {chapter_title}"

        # 2. Detect B√†i h·ªçc (lesson)
        lesson_match = re.search(
            r"B√ÄI\s+(\d+)[:Ôºö\.\s-]*([^\n\r]+)",
            lesson_text,
            re.IGNORECASE
        )
        if lesson_match:
            lesson_num = int(lesson_match.group(1))
            lesson_title = re.sub(r'\s+', ' ', lesson_match.group(2).strip()).title()
            metadata["lesson_number"] = lesson_num
            metadata["lesson_title"] = lesson_title

        # 3. Detect m·ª•c ti√™u h·ªçc t·∫≠p (topics/learning objectives)
        metadata["topics"] = self._extract_learning_objectives(lesson_text)

        # 4. Detect sections (Ho·∫°t ƒë·ªông, ph·∫ßn n·ªôi dung)
        metadata["sections"] = self._extract_sections(lesson_text)

        # 5. Detect content types
        metadata["has_questions"] = bool(re.search(r"C√¢u h·ªèi\s*\d*[:Ôºö]?", lesson_text, re.IGNORECASE))
        metadata["has_activities"] = bool(re.search(r"HO·∫†T ƒê·ªòNG\s*\d*[:Ôºö]?", lesson_text, re.IGNORECASE))
        metadata["has_exercises"] = bool(re.search(r"LUY·ªÜN T·∫¨P|V·∫¨N D·ª§NG", lesson_text, re.IGNORECASE))
        metadata["has_code"] = any(kw in lesson_text for kw in ["def ", "class ", "print(", "import ", "```"])
        metadata["has_formula"] = any(op in lesson_text for op in ["=", "+", "-", "√ó", "√∑", "‚àö"])
        metadata["has_diagram"] = any(k in lesson_text.lower() for k in ["h√¨nh", "s∆° ƒë·ªì", "bi·ªÉu ƒë·ªì", "ƒë·ªì th·ªã"])

        # 6. Count special blocks
        metadata["code_blocks_count"] = lesson_text.count("```")
        metadata["question_count"] = len(re.findall(r"C√¢u h·ªèi\s*\d+[:Ôºö]?", lesson_text, re.IGNORECASE))
        metadata["activity_count"] = len(re.findall(r"HO·∫†T ƒê·ªòNG\s*\d+[:Ôºö]?", lesson_text, re.IGNORECASE))

        return metadata

    def _extract_learning_objectives(self, text: str) -> List[str]:
        """Extract m·ª•c ti√™u h·ªçc t·∫≠p t·ª´ ph·∫ßn 'Sau b√†i n√†y em s·∫Ω:'"""
        objectives = []

        # T√¨m ph·∫ßn m·ª•c ti√™u
        goal_section = re.search(
            r"Sau b√†i (?:n√†y )?em (?:s·∫Ω|c·∫ßn):\s*([\s\S]*?)(?=\n\n[A-Z]|\nKH·ªûI ƒê·ªòNG|\nN·ªòI DUNG|$)",
            text,
            re.IGNORECASE
        )

        if goal_section:
            goal_text = goal_section.group(1)
            # T√¨m c√°c d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng * ho·∫∑c -
            for line in goal_text.split('\n'):
                line = line.strip()
                if line.startswith(('*', '-', '‚Ä¢')):
                    clean = re.sub(r'^[\*\-‚Ä¢]\s*', '', line).strip()
                    if 10 <= len(clean) <= 200:
                        objectives.append(clean)

        return objectives[:10]  # Gi·ªõi h·∫°n 10 objectives

    def _extract_sections(self, text: str) -> List[Dict[str, str]]:
        """
        Extract c√°c sections (ph·∫ßn) trong b√†i h·ªçc
        VD: KH·ªûI ƒê·ªòNG, HO·∫†T ƒê·ªòNG 1, N·ªòI DUNG, LUY·ªÜN T·∫¨P, V·∫¨N D·ª§NG
        """
        sections = []

        # Patterns cho c√°c sections ph·ªï bi·∫øn
        section_patterns = [
            r"(KH·ªûI ƒê·ªòNG)(?:\s*[:Ôºö])?\s*([^\n]*)",
            r"(HO·∫†T ƒê·ªòNG)\s*(\d+)[:Ôºö]?\s*([^\n]+)",
            r"(LUY·ªÜN T·∫¨P)(?:\s*[:Ôºö])?\s*([^\n]*)",
            r"(V·∫¨N D·ª§NG)(?:\s*[:Ôºö])?\s*([^\n]*)",
            r"(\d+)\.\s+([A-Z√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√ö√ù][^:\n]{5,80})",  # Numbered sections
        ]

        for pattern in section_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                if len(match.groups()) == 2:
                    section_type = match.group(1).strip()
                    section_title = match.group(2).strip() if match.group(2) else ""
                elif len(match.groups()) == 3:
                    section_type = f"{match.group(1)} {match.group(2)}"
                    section_title = match.group(3).strip()
                else:
                    continue

                sections.append({
                    "type": section_type,
                    "title": section_title,
                    "position": match.start()
                })

        # Sort by position
        sections.sort(key=lambda x: x['position'])

        # Remove position before returning
        for s in sections:
            del s['position']

        return sections

    def _smart_chunk_by_sections(self, text: str, metadata: Dict) -> List[Dict]:
        """
        Chunk b√†i h·ªçc d·ª±a tr√™n sections
        M·ªói chunk l√† m·ªôt section ho·∫∑c m·ªôt ph·∫ßn c·ªßa section n·∫øu qu√° d√†i
        """
        chunks = []
        sections = metadata.get('sections', [])

        if not sections:
            return self._basic_chunk(text, metadata)

        # T√¨m v·ªã tr√≠ c·ªßa c√°c sections trong text
        section_positions = []
        for section in sections:
            section_marker = f"{section['type']}"
            if section['title']:
                section_marker += f": {section['title']}"

            pos = text.find(section_marker)
            if pos != -1:
                section_positions.append((pos, section))

        section_positions.sort(key=lambda x: x[0])

        # Extract text cho t·ª´ng section
        for i, (pos, section) in enumerate(section_positions):
            next_pos = section_positions[i+1][0] if i+1 < len(section_positions) else len(text)
            section_text = text[pos:next_pos].strip()

            # N·∫øu section qu√° d√†i, chia nh·ªè
            if len(section_text) > self.chunk_size * 1.5:
                sub_chunks = self._basic_chunk(section_text, metadata)
                for sub_chunk in sub_chunks:
                    sub_chunk['section_type'] = section['type']
                    sub_chunk['section_title'] = section.get('title', '')
                    chunks.append(sub_chunk)
            else:
                chunk = {
                    'content': section_text,
                    'metadata': metadata.copy(),
                    'token_count': self._count_tokens(section_text),
                    'section_type': section['type'],
                    'section_title': section.get('title', '')
                }
                chunks.append(chunk)

        return chunks

    def _basic_chunk(self, text: str, metadata: Dict) -> List[Dict]:
        """Chunk c∆° b·∫£n s·ª≠ d·ª•ng RecursiveCharacterTextSplitter"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""],
        )

        chunks = []
        for chunk_text in splitter.split_text(text):
            chunk = {
                'content': chunk_text,
                'metadata': metadata.copy(),
                'token_count': self._count_tokens(chunk_text)
            }
            chunks.append(chunk)

        return chunks

    def _detect_subject(self, filename: str, text: str = "") -> Dict:
        """Detect subject from filename or content"""
        name = filename.lower()

        if "tin" in name or "tin h·ªçc" in text.lower():
            return {"subject": "Tin h·ªçc", "subject_key": "tin_hoc"}
        elif "toan" in name or "to√°n" in text.lower():
            return {"subject": "To√°n h·ªçc", "subject_key": "toan"}
        elif "van" in name or "ng·ªØ vƒÉn" in text.lower():
            return {"subject": "Ng·ªØ vƒÉn", "subject_key": "ngu_van"}

        return {"subject": "Unknown", "subject_key": "unknown"}

    def _detect_grade(self, filename: str) -> Tuple[Optional[int], Optional[str]]:
        """Detect grade level from filename"""
        match = re.search(r"(\d+)", filename)
        grade = int(match.group(1)) if match else None

        if grade:
            if grade <= 5:
                level = "Ti·ªÉu h·ªçc"
            elif grade <= 9:
                level = "THCS"
            else:
                level = "THPT"
        else:
            level = None

        return grade, level

    def _count_tokens(self, text: str) -> int:
        """Count tokens using tiktoken"""
        return len(self.encoding.encode(text))

    def _save_chunks(self, chunks: List[Dict], output_file: Path):
        """Save chunks to JSON file"""
        import json
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)


def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(description="üìò Process textbook files into JSON chunks")
    parser.add_argument("--input", required=True, help="Input TXT file path")
    parser.add_argument("--output", required=True, help="Output folder")
    parser.add_argument("--type", choices=["txt", "pdf"], default="txt", help="File type (txt or pdf)")
    parser.add_argument("--subject", default=None, help="Subject (tin_hoc, toan, etc.)")
    parser.add_argument("--chunk-size", type=int, default=1000, help="Chunk size")
    parser.add_argument("--chunk-overlap", type=int, default=200, help="Chunk overlap")
    parser.add_argument("--no-smart-chunking", action="store_true", help="Disable smart chunking")

    args = parser.parse_args()

    processor = DocumentProcessor(
        subject=args.subject,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        smart_chunking=not args.no_smart_chunking
    )

    path = Path(args.input)

    if args.type == "txt":
        if path.is_dir():
            for txt_file in path.glob("*.txt"):
                processor.process_txt(txt_file, args.output)
        else:
            processor.process_txt(path, args.output)
    elif args.type == "pdf":
        logger.error("PDF processing not implemented yet. Please use TXT files.")
        return


if __name__ == "__main__":
    main()