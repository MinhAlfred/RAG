# ðŸš€ Optimization Guide - Cloud Deployment

**Reduce Docker image size by ~60% and deployment time significantly!**

## ðŸ“Š Before vs After

| Metric | Before (Full) | After (Optimized) | Savings |
|--------|---------------|-------------------|---------|
| **Docker Image** | ~2.5 GB | ~900 MB | **-64%** |
| **Build Time** | ~8-10 min | ~3-5 min | **-50%** |
| **Dependencies** | 77 packages | ~45 packages | **-42%** |
| **System Packages** | 12 packages | 2 packages | **-83%** |

## âŒ What We Removed

### 1. CUDA/GPU Dependencies (~1.2 GB)
**Reason**: You're using `EMBEDDING_DEVICE=cpu`

```bash
# Removed:
torch (CUDA version) - ~1.2 GB
# Replaced with:
torch (CPU-only) - ~100 MB
```

### 2. OCR Dependencies (~300 MB)
**Reason**: `OCR_ENABLED=False` in your settings

```bash
# Removed:
- pytesseract (~50 MB)
- pdf2image (~20 MB)
- opencv-python (~200 MB) - Heavy computer vision library
- tesseract-ocr (system package)
- poppler-utils (system package)
- libgl1-mesa-glx (OpenGL for OpenCV)
```

### 3. Local LLM Dependencies
**Reason**: You're using Gemini API (cloud)

```bash
# Removed from imports (but kept in code for flexibility):
- Ollama integration (using Gemini instead)
```

### 4. Unused Vector Stores
**Reason**: Using Qdrant Cloud only

```bash
# Removed:
- chromadb (~100 MB)
```

### 5. Unused LLM APIs
**Reason**: Using Gemini only

```bash
# Removed:
- openai
- tiktoken (OpenAI tokenizer)
- langchain-openai
```

## âœ… What We Kept (Required)

### 1. Embeddings (Local Processing)
```bash
âœ… sentence-transformers - For creating embeddings
âœ… torch (CPU-only) - Required by sentence-transformers
âœ… transformers - Required by sentence-transformers
```

**Why?**: Even though you use cloud LLM (Gemini), embeddings run locally:
- Converts user queries to vectors
- Uses `paraphrase-multilingual-MiniLM-L12-v2` model
- Needs torch for model inference

### 2. Cloud Services
```bash
âœ… langchain-google-genai - Gemini API
âœ… google-generativeai - Gemini SDK
âœ… qdrant-client - Qdrant Cloud
âœ… asyncpg/psycopg2 - Supabase PostgreSQL
```

### 3. Core Framework
```bash
âœ… FastAPI, uvicorn
âœ… LangChain ecosystem
âœ… SQLAlchemy
```

## ðŸ”„ Migration Steps

### Step 1: Use Optimized Requirements

```bash
# Build with optimized requirements
docker-compose -f docker-compose.cloud.yml build

# This will use:
# - Dockerfile.cloud (optimized)
# - requirements-cloud.txt (minimal dependencies)
```

### Step 2: Verify Environment

Make sure your `.env` has:
```bash
# CPU-only (no CUDA)
EMBEDDING_DEVICE=cpu

# Cloud services
LLM_TYPE=gemini
QDRANT_URL=https://...
# Supabase credentials
```

### Step 3: Deploy

```bash
make cloud-build
make cloud-up
make cloud-health
```

## ðŸ” Detailed Changes

### requirements.txt â†’ requirements-cloud.txt

#### Removed Packages:
```python
# OCR Processing (not needed)
- pdfplumber>=0.11.7
- pdf2image>=1.17.0
- pytesseract>=0.3.13
- opencv-python>=4.10.0

# Local Vector Stores (using Qdrant Cloud)
- chromadb>=0.6.3

# OpenAI (using Gemini)
- openai>=1.59.5
- tiktoken>=0.8.0
- langchain-openai>=1.0.0
```

#### Modified Packages:
```python
# Before: Full PyTorch with CUDA (~2.2 GB)
torch>=2.5.1,<3.0.0

# After: CPU-only PyTorch (~100 MB)
--extra-index-url https://download.pytorch.org/whl/cpu
torch>=2.5.1,<3.0.0
```

### Dockerfile â†’ Dockerfile.cloud

#### Removed System Packages:
```dockerfile
# Before:
RUN apt-get install -y \
    build-essential \
    git \
    tesseract-ocr \           # OCR âŒ
    tesseract-ocr-vie \       # OCR âŒ
    tesseract-ocr-eng \       # OCR âŒ
    poppler-utils \           # PDF to image âŒ
    libgl1-mesa-glx \         # OpenGL for OpenCV âŒ
    libglib2.0-0              # Still needed âœ…

# After:
RUN apt-get install -y \
    curl                      # Only for healthcheck
```

## ðŸ“ˆ Performance Impact

### Build Time
```bash
# Before (Full)
Building rag-api ... done (8m 32s)

# After (Optimized)
Building rag-api-cloud ... done (3m 45s)
```

### Image Size
```bash
# Before
docker images
REPOSITORY    TAG    SIZE
rag-api      latest  2.47 GB

# After
docker images
REPOSITORY         TAG    SIZE
rag-api-cloud     latest  890 MB
```

### Startup Time
```bash
# Before: ~45-60 seconds
# After: ~20-30 seconds
```

### Memory Usage
```bash
# Before: ~1.5 GB at idle
# After: ~600 MB at idle
```

## âš ï¸ Important Notes

### 1. You Still Need Torch (CPU version)

**Common Misconception**: "I use cloud LLM, so I don't need torch"

**Reality**: You use local embeddings!

```python
# From src/sgk_rag/core/embedding_manager.py
HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'},  # â† Still runs locally!
)
```

### 2. OCR Can Be Re-enabled

If you need OCR later, use the full `requirements.txt`:

```bash
# For PDF processing with OCR
docker-compose build  # Uses original Dockerfile
```

### 3. Multi-Stage Builds

Current setup uses single-stage. For even smaller images:

```dockerfile
# Stage 1: Build dependencies
FROM python:3.11-slim as builder
# ... build wheels

# Stage 2: Runtime (copy only wheels)
FROM python:3.11-slim
# ... copy from builder
```

## ðŸ§ª Testing the Optimized Build

### 1. Build Optimized Image

```bash
make cloud-build
```

### 2. Test Locally

```bash
# Start service
make cloud-up

# Wait for startup
sleep 30

# Test health
curl http://localhost:8000/health | jq

# Test embedding (uses torch)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $RAG_API_KEY" \
  -d '{"question": "Python lÃ  gÃ¬?"}' | jq
```

### 3. Verify Dependencies

```bash
# Check torch version (should be CPU-only)
make cloud-shell
python -c "import torch; print(f'Torch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"

# Should output:
# Torch: 2.5.1+cpu
# CUDA: False
```

### 4. Check Image Size

```bash
docker images rag-api-cloud
# Should be ~900 MB
```

## ðŸ”„ Rollback Plan

If you need to rollback to full version:

```bash
# Use original docker-compose
docker-compose build
docker-compose up -d

# Or specify full Dockerfile
docker-compose -f docker-compose.yml up -d
```

## ðŸ“š File Structure

```
RAG/
â”œâ”€â”€ requirements.txt           # Full version (all dependencies)
â”œâ”€â”€ requirements-cloud.txt     # Optimized (cloud deployment) â­
â”œâ”€â”€ Dockerfile                 # Full version (multi-stage)
â”œâ”€â”€ Dockerfile.cloud          # Optimized (single-stage) â­
â”œâ”€â”€ docker-compose.yml        # Local dev (with Qdrant container)
â””â”€â”€ docker-compose.cloud.yml  # Cloud (no database containers) â­
```

## ðŸŽ¯ Recommendations

### For Development
```bash
# Use full version for flexibility
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up
```

### For Production/Cloud
```bash
# Use optimized version for speed & size
docker-compose -f docker-compose.cloud.yml up -d
```

### For PDF Processing (One-time)
```bash
# Process PDFs locally with full dependencies
python scripts/create_vectorstore_all.py

# Then deploy optimized version
make cloud-build && make cloud-up
```

## âœ… Verification Checklist

After deploying optimized version:

- [ ] Docker image size < 1 GB
- [ ] Health check passing (`make cloud-health`)
- [ ] Embeddings working (test /ask endpoint)
- [ ] Gemini API connected
- [ ] Qdrant Cloud connected
- [ ] Supabase connected
- [ ] Memory usage < 800 MB
- [ ] Startup time < 40 seconds

## ðŸ†˜ Troubleshooting

### "ModuleNotFoundError: No module named 'cv2'"

**Solution**: You're trying to use OCR. Use full Dockerfile:
```bash
docker-compose -f docker-compose.yml build
```

### "torch: undefined symbol"

**Solution**: Clear Docker cache and rebuild:
```bash
make cloud-build-no-cache
```

### "CUDA not available" (Expected!)

This is **normal** for CPU-only version. Your embeddings will run on CPU.

---

**Optimized for Cloud! ðŸš€**
